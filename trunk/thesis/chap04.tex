\chapter{Simplificação de Superfícies}
Como mencionado anteriormente (ver capítulo \ref{cap:02}), superfícies baseadas
em pontos descrevem modelos complexos usando milhões ou mesmo bilhões de amostras (por
exemplo \cite{Levoy2000}). Reduzir a complexidade deste modelos é uma das
principais técnicas de processamento para desenvolvimento de algoritmos de
modelagem e visualização. Simplificação de superfícies provê mecanismos para
gerar uma aproximação adequada da superfície com menos amostras que o modelo
original. Essa aproximação deve se assemelhar o máximo possível do original.
Formalmente, o objetivo de uma simplificação de superfícies baseadas em pontos
pode ser formuladas como se segue: Seja uma superfície $\mathbf{\mathcal{S}}$
definida por uma nuvem de pontos $\mathbf{\mathcal{P}}$. Dado um número
amostras $n<|\mathbf{\mathcal{P}}|$, compute um nuvem de pontos
$\mathbf{\mathcal{P}}^\prime$ onde $|\mathbf{\mathcal{P}^}^\prime| = n$ tal que
a distância $\mathbf{\varepsilon} =
\mathbf{\mathit{d}}(\mathbf{\mathcal{S}},\mathbf{\mathcal{S}}^\prime)$ da
superfície simplificada $\mathbf{\mathcal{S}}^\prime$ da superfície original 
$\mathbf{\mathcal{S}}$ seja mínima.

%Na prática ,  achar um ótimo global do problema descrito acima é $np - hard$
Muitos dos algoritmos de simplificação existentes usam
diferentes heurísticas baseadas em erros locais.
Neste capítulos serão apresentados alguns métodos de simplificação de
superfícies baseadas em pontos. Eles serão divididos em duas categorias:
Superfícies baseadas em pontos \textit{Puros} e baseadas em \textit{Splats}.

%{New point based simplification based on MLS and splat for Point Modesl}

\section{Simplificação Baseada em Pontos \textit{Puros}}
\label{sec:puro}
Uma abordagem simples e muito usada na simplificação de nuvem de pontos é
avaliar a importância de cada ponto na nuvem de pontos. A importância é
qualificada por um valor que indica a quantidade de informação que o ponto
possui para formar a superfície ou sua redundância. A simplificação é então
realizada removendo os pontos que não contribuem significativamente ou são
muito redundantes. Linsen \cite{Linsen2001} defini a informação contida em um
ponto como uma soma ponderada de fatores que inclui distância, curvatura e
etc, estimada pelos seus vizinhos. Em Alexa et al.\cite{Alexa2001} os
pontos são removidos de acordo com importância de sua contribuição
na representação da superfície dada por \texit{MLS} ( \texit{Moving Least
Squares}\abbrev{MLS}{\texit{Moving Least Squares}}). Kalaiah e Varshney
\cite{Kalaiah2003} medem a importância de cada ponto baseado nas propriedades 
diferencias fornecidas pela sua vizinhança. 

\begin{figure}[ht]
\centering
\begin{tabular}{cc}
\includegraphics[width=7.0cm]{img/cap04/teaPotSong} &
\includegraphics[width=6.0cm]{img/cap04/pecaSong} 
\end{tabular}
\caption{Método de simplificação proposto por~\cite{Song2009}. Nos dois
modelos: \textbf{Topo-esquerda:} Modelo original. \textbf{Topo-direta:} Em
destaque pontos, classificados como de bordas. \textbf{Baixo:} Modelos
simplificado.}
\label{fig:song2009}
\end{figure}

Os métodos de simplificação citados acima são baseados na suposição de que a
superfície determinada pela nuvem de pontos é suave. Modelos de peças mecânicas
(?)contudo, contém bordas afiadas (\ldots) que separa a superfície em duas
partes geralmente distintas. Song e Feng \cite{Song2009} criaram um método de
simplificação de modelos com bordas afiadas que usa a mesma ideia do métodos
acima citados. O método de simplificação proposto por eles difere dos método de
Kalaiah e Varshney \cite{Kalaiah2003} e  Alexa et al.\cite{Alexa2001} em duas
formas. Primeiro, ao invés de operar sobre toda a nuvem de pontos de uma só vez,
 o algoritmo operar apenas nas regiões suaves. Segundo, o grau de importância 
 dos pontos é medido de forma diferente. Enquanto Alexa et al.\cite{Alexa2001} focam em uma simplificação
continua, e Kalaiah e Varshney \cite{Kalaiah2003} focam na renderização e em
uma distribuição homogênea, Song e Feng \cite{Song2009} focam em simplificação
que preserve as partes característica do modelo como ilustrado na
Figura~\ref{fig:song2009}.

%\begin{figure}[ht]
%\centering
%\includegraphics[width=15.0cm]{img/cap04/MaxPlanckPauly} 
%\caption{\textbf{Topo-Esquerda:} \ldots \textbf{Baixo-Esquerda:} \ldots
%\textbf{Topo-Direita:} \ldots \textbf{Baixo-Direta:}}
%\label{fig:maxplanck}
%\end{figure} 



\begin{figure}[ht]
\centering
\begin{tabular}{cc}

	\begin{minipage}[b]{0.45\linewidth}
    	\centering
 		\includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinCluster}\\[0cm](a)
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.45\linewidth}
    	\centering
 		\includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinBSP}\\[0cm](b)
   \end{minipage}
	\\
	
  \begin{minipage}[b]{0.45\linewidth}
  		\centering
        \includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinParticulas}\\[0cm](c)
  \end{minipage}  
  \hfill
  \centeringmaxPlanckWinCluster
  \begin{minipage}[b]{0.45\linewidth}
    	\centering
        \includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinParticulasQEM}\\[0cm](d)
  \end{minipage}
	
\end{tabular}
\caption{Métodos de simplificação propostos por~\cite{Pauly2002}. \textbf{(a)}
Clusterização Incremental. \textbf{(b)} Clusterização Hierárquica. \textbf{(c)} 
Simulação de Partículas. \textbf{(d)} Simplificação Interativas.}
\label{fig:pauly}
\end{figure}
 
Os algoritmos citados acima criam uma nuvem de pontos simplificada que é na
verdade um subconjunto da nuvem de pontos original, onde interativamente removem pontos 
de acordo com uma métrica erro . 

%Moenning e Dodgson \cite{Moenning03} aplicou um método que usa uma abordagem
%progressiva que começando com um ponto escolhido randomicamente, acrescentamos
%a cada passo no conjunto de representantes já selecionados a partir de amostras
%que mais distancie deles. Esse ponto é sabidamente um vértices do driagrama de
%Voronoi (geodésico) do conjunto de pontos já selecionados. Como a determinação
%desse diagrama é custosa ele os autores propoe substitui-lo por uma
%aproximação usando \textit{Fast-Marching Farthest Point} que é descrito a
%seguir:
%\begin{enumerate}
%\item Em primeiro lugar se imerge a nuvem de pontos em um grade regular.
%\item A distância dessa grade ao conjunto de pontos é calculada de forma
%iterativa a partir da distância dos vértices vizinhos que estão mais próximos
%\item da superfície $\mathbf{\mathcal S}$ que ele.
%\item Células contendo vértices cujos pontos mais próximos em $\mathbf{\mathcal
%S}$ são diferentes, contém faces do diagrama de Voronoi. Dentro dessas células
%essas faces são aproximados por um polígono planar.
%\end{enumerate} 

%Na verdade só precisamos tratar as células adjacentes ao vértice mais distante
%de $\mathbf{\mathcal S}$.

%aplicou o princípio de \textit{Fast-Marching Farthest
%Point}, introduzida por Eldar et al.\cite{Eldar1997} no contexto de amostragem
%em
%imagens. O método ultiliza um abordagem progressiva que começa com 
 
 
 %que não requer nenhuma reconstrução de superfícies a priore. O
 %Algoritmo pode gerar uma representação em multiresolução e
 %progressiva de forma eficiente.

Enquanto estes métodos são simples e eficientes, suas estratégias de
simplificação não garantem uma distribuição global uniforme. Além disso, os
erros proposto por estes métodos não produzem um boa estimativa e pode não ser
trivial formular uma métrica mais precisa.

Um maneira sistemática de diferentes abordagens de simplificação de pontos foi
proposta por Pauly et al.\cite{Pauly2002}, onde vários métodos de simplificação
de polígonos foram adaptados para pontos. São eles:
\begin{itemize}
  \item{\textit{Métodos de Clusterização:} Onde a nuvem de pontos
  $\mathbf{\mathcal P}$ é dividida em subconjuntos. Os pontos deste subconjun
 to são então subistituido por um ponto representativo (geralmente seu
 centroide). Existem dois tipos de clusterização. Incremental, onde clusters
 são criados sistematicamente crescendo regiões através da agregação de pontos
vizinhos, e o hierárquico onde clusters são criados tomando a
nuvem de pontos original com um todo e dividindo-a em conjuntos menores.}
  \item{\textit{Simplificação Iterativa:} Onde repetidamente seleciona-se pares
  de pontos e junta-os em um só. Note que a cada junção, há um acréscimo no
  erro total. No entanto, cria-se uma lista ordenada com todas as possíveis
  junções, onde a junção que causa o menor erro fica no topo. Então pares de
  pontos são aglomerados de forma que o erro seja mínimo. Para avaliar o
  erro de junção dos pares de pontos é usada uma adaptação do \texit{Quadric
  Error Metric}\abbrev{QEM}{\texit{Quadric Error Metric}} apresentado para
  malhas poligonais em~\cite{Garland1997}.}
  \item{\textit{Simulação de Partículas:} Onde há uma distribuição aleatória
  de um número $\mathbf{n}$ de pontos na superfície, tal que
  $|\mathbf{\mathcal{P}^}^\prime| = n$. Então cada ponto é considerado um partícula 
  que aplica forças de repulsão uma às outras, movendo-se até que a
  distribuição desejada seja alcançada.}
\end{itemize}
  
Como discutido em \cite{Pauly2002}, o método de clusterização incremental é o
que possui o maior erro médio, seguido do método hierárquico, simulação de
partículas e do método iterativo, que possui menor erro. A distribuição dos
pontos produzida pelos métodos de clusterização é próximo da distribuição
original, que é desejável em alguns caso. Entretanto, simulação de
partículas é a alternativa mais viável quando se deseja um densidade uniforme ou um controle
da densidade local da distribuição. Métodos de clusterização são os mais
eficientes devido a sua simples. Clusterização 
incremental, simplificação iterativa e simulação de partículas precisam de memória extra 
que é linear no número de amostras, enquanto a
clusterização hierárquica depende apenas do tamanho do modelo simplificado. O
autor discute que as três técnicas operam diretamente na nuvem de pontos, ao
invés de reconstruir a superfície a priori e só então aplicar a simplificação.
A Figura~\ref{fig:pauly} ilustra estes métodos.

\section{Simplificação Baseada em \textit{Splats}}

Como discutido no capítulo \ref{cap:02}, para preencher buracos entre os pontos
de forma mais eficiente, representações baseadas em pontos são frequentemente
estendidas para representações baseadas em \textit{Splats}~\cite{Zwicker2001},
onde a superfície é aproximada por discos ou elipses. Este representação é de
especial interesse no contexto de renderização, onde os \textit{splats}
facilitam a concepção de algoritmos
eficientes~\cite{Ren2002,BotschW2002,Botsch2003,marroquim07}.

Embora representação baseada em \textit{splats} são geralmente usados na
prática, muitos esquemas de simplificação tomam apenas o centro do
\textit{splat} em consideração (como discutido na anterior).
A ideia é primeiro gerar o conjunto de pontos na qual a densidade se adapta a
curvatura da superfície. Em um processo posterior estes pontos são convertidos
em \textit{splats} pela analise local de sua vizinhança, e assim estimando sua
normal e seu tamanho. Em contraste, técnicas de simplificação
baseadas em \textit{splat}, exploram toda a geometria do disco ou elipse para
obter o conjunto de \textit{splats} que recobrem a superfície. No entanto, estas
técnicas são computacionalmente mais trabalhosas. 

Wu e Kobbelt~\cite{WuK04} desenvolveram um técnica de simplificação
otimizada baseada em \textit{splats} que explora a flexibilidade de modelos de
pontos, ou seja, sem informação de topologia, com uma esquema de otimização global que computa um
aproximação de toda a superfície com um conjunto mínimo de \textit{splats} que
satisfaz um erro global pré-determinado. 


\begin{figure}[ht]
\centering
\includegraphics[width=15.0cm]{img/cap04/wu04Iteractive} 
\caption{ \textbf{Esquerda:} Modelo de um torso feminino $171.000$ pontos.
\textbf{Meio:} Modelo aproximado por $422$ \textit{splats} após o segundo passo
(seleção gulosa). \textbf{Direita:} $333$ \textit{splats} após
finalizado o terceiro passo (otimização global usando sistemas de partículas)
\textbf{Direita-Meio} e \textbf{Esquerda-Meio} são a renderização
dos \textit{splats} usando filtro EWA. Note a melhor distribuição dos
\textit{splats} após passo de otimização global}
\label{fig:wu04}
\end{figure} 


O objetivo da simplificação otimizada é computar um conjunto mínimo de
\textit{splats} $\mathbf{\mathcal{T}=\{\mathbf{s}_j}\}$ que aproxime o conjunto
original $\mathbf{\mathcal{P}}$ com um erro abaixo de uma tolerância
$\mathbf{\varepsilon}$. Para inicializar o algoritmo, ao longo da sua relação
de vizinhança dada pelos $k$-vizinhos, para cada ponto ${\mathbf{p}_i}$,
propriedades locais da superfície como normal $\mathbf{n}_i$ e 
densidade $\mathbf{\omega}_i$ tem que ser calculados de ante mão. Dependendo da
aplicação, pode-se escolher entre \textit{splats} circulares ou elípticos. Um
\textit{splat} circular e definido por um centro ${\mathbf{c}_i}$, normal
${\mathbf{n}_i}$ e um raio ${\mathbf{r}_i}$. No caso elíptico o raio
${\mathbf{r}_i}$ é substituído por dois vetores ${\mathbf{t}_i^1}$ e
${\mathbf{t}_i^2}$ que definem os eixos maior e menor da elipse.
Para assegurar o erro $\mathbf{\varepsilon}$, uma nova métrica de distância
\textit{splat}-ponto é introduzida. Em outras palavras, para um ponto
${\mathbf{p}_i}$ a distância para o \textit{splat} ${\mathbf{\mathcal T}}$ é
definida como sendo a projeção ortogonal no \textit{splat} ${\mathbf{s}_j}$:
\begin{equation}
 dist(\mathbf{p}_i,\mathbf{\mathcal T}) = dist(\mathbf{p}_i,\mathbf{s}_j) =
 |\mathbf{n}_i^{\mathit T}(\mathbf{p}_i - \mathbf{c}_i)|
\label{eq1}
\end{equation}
\noindent se
\begin{equation}
 || (\mathbf{p}_i - \mathbf{c}_i) - \mathbf{n}_j^{\mathit T}(\mathbf{p}_i -
 \mathbf{c}_i) - \mathbf{n}_j ||^2 \leq \mathbf r_j ^2
\label{eq:circular}
\end{equation}
para \textit{splat} circular ou
\begin{equation}
 (  \mathbf{t}_j^{1^{\mathit T}}(\mathbf{p}_i - \mathbf{c}_i) )^2  + 
 (\mathbf{t}_j^{2^{\mathit T}}(\mathbf{p}_i - \mathbf{c}_i))^2 \leq 1
\label{eq:eliptico}
\end{equation}
se for \textit{splat} elíptico. Se $\mathbf{p}_i$ projeta no interior de vários
\textit{splats}, a distância mínima é escolhida. Se equações \ref{eq:circular}
ou \ref{eq:eliptico} não retornarem nenhum $\mathbf{s}_j$, sua distância e
tomada como $dist(\mathbf{p}_i,\mathbf{\mathcal T}) = \infty$.
 
A simplificação otimizada segue três passos. Primeiro passo é computar o
\textit{splat} máximo para cada ponto ${\mathbf{p}_i}$ cujo erro é limitado por
um $\mathbf{\varepsilon}$. Começando por uma semente $\mathbf{p}_i$, 
o \textit{splat} $\mathbf{s}_i$ cresce adicionando os vizinhos
de acordo com sua distancias calculada usando \ref{eq:circular} para
\textit{spalt} circular ou \ref{eq:eliptico} para elíptico. Este último
adapta-se melhor a curvatura da superfície. Em um segundo passo, para o conjunto inicial de \textit{splats}, um subconjunto
que cobre a superfície sem que haja buracos entre os vizinhos é selecionado
usando um procedimento guloso.
 
Por último, o procedimento guloso anterior é otimizado usando um procedimento
global. A ideia e iterativamente substituir subconjuntos de \textit{splats}
por um novo subconjunto que tenha mesmo elementos ou um que tenha um melhor
distribuição, já que o procedimento anterior é baseado em decisões locais e
geralmente redundante ou possuindo um distribuição desigual. A ideia é usar um
sistemas de partículas como em~\cite{Pauly2002,Tuk}, diferenciando na forma
como as partículas interagem. A figura~\ref{fig:wu04} mostra o
efeito desta simplificação otimização quando o último passo é finalizado.

Wu el al.~\cite{ProgressiveSplat2005} desenvolveram um algoritmo iterativo com
um abordagem gulosa para criar uma representação progressiva de \textit{splats}.
Funciona de forma semelhante ao \textit{Progressive Meshes} de
Hoppe~\cite{Hoppe1996}. Especificamente, data um conjunto de pontos são criado
\textit{splats} para cada deles. Em seguida, todas as possíveis operações de
\textit{junção de} \texti{splats} são organizada em um lista de prioridades de acordo com uma \textit{métrica de
erro} que avalia o erro causado pela respectiva operação de junção com o
elemento com menor erro estando no topo.

\begin{figure}[ht]
\centering
\includegraphics[width=15.0cm]{img/cap04/progressiveSplat} 
\caption{ Representação progressiva do modelo de Carlos Magno ($600.000$
 de pontos). Da esquerda para direita, modelos com: $20.00$,
$10.000$, $70.000$ e $600.000$ \textit{splats}}
\label{fig:progressiveSplat}
\end{figure} 

A entrada do algoritmo é um conjunto de pontos $\mathbf{\mathcal P}
= \{\mathbf{1}\ldots\mathbf{p}_i\}$ que são transformado em um conjunto
\textit{splats} $\mathbf{\mathcal T  = \{\mathbf{1} \ldots \mathbf{s}_i}\}$ onde
cada \textit{splat} representa um elipse em $3D$ com centro $\mathbf{c}_i$, normal
$\mathbf{n}_i$ e dois vetores ortogonais $\mathbf{t}_i^1$ e $\mathbf{t}_i^2$

Similar a~ \cite{Pauly2002} e \cite{WuK04} os $k$-vizinhos de $\mathbf{p}_i$, 
$\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$,   
são computados de ante mão, tanto para analisar a superfície local 
(computar normal $\mathbf{n}_i$) como para gerar o splat inicial $\mathbf{s}_i$.
Para tanto, um plano $\mathbf{\mathcal{H}}$ é formado para $\mathbf{p}_i$ e
$\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$ definindo $\mathbf{s}_i$ com normal 
$\mathbf{n}_i$ e centro $\mathbf{c}_i = \mathbf{p}_i$. Como os
\textit{splats} iniciais são círculos, $\mathbf{t}_i^1$ e $\mathbf{t}_i^2$
serão dois vetores ortogonais  paralelos a $\mathbf{\mathcal{H}}$ com mesmo
tamanho $\mathbf{r}_i$,

\begin{equation}
 \mathbf{r}_i  = \max_j || (\mathbf{p}_j - \mathbf{c}_i) - \mathbf{n}_i^{\mathit
 T}(\mathbf{p}_j - \mathbf{c}_i) - \mathbf{n}_i || ,
\label{eq:circular}
\end{equation}

\noindent para todo  $\mathbf{p}_j$ $\in$
$\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$. Assim como feito por
\cite{Pauly2002}, $\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$ será usado para criar
uma topologia dinâmica para auxiliar o processo de junção de \textit{splats}.
Sendo assim, é criado um grafo
$\mathbf{\mathcal{G}}(\mathbf{\mathcal{P}},\mathit{E})$ onde uma aresta $(i,j)$ 
pertence a $\mathit{E}$ se $\mathbf{p}_j \in \mathbf{\mathcal{N}}_k(\mathbf{p}_i)$.
Então um operação de junção de \textit{splat} $\Phi$ juntará dois
\textit{splats} $\mathbf{s}_a$ e $\mathbf{s}_b$ associados a dois pontos
$\mathbf{p}_a$ e $\mathbf{p}_b$ de um aresta $\mathbf{e} \in \mathit{E}$
em um \textit{splat} $\mathbf{s}_m$. Assim,  a lista de prioridades conterá todos as
possíveis operações de junção de todas as arestas pertencentes à $\mathit{E}$.  
Para avaliar a operação de junção dois \texit{splats}, dois erros foram
entendidos. Um que mede o erro pela distância ($\mathbf{\mathit{L}}^2$) e
outro pelo desvio da normal ($\mathbf{\mathit{L}}^{2,1}$).

\subsubsection{Métrica $\mathbf{\mathit{L}}^2$}

A métrica $\mathbf{\mathit{L}}^2$ é baseada na distância Euclidiana. Para
computar o erro causado pela operação de junção de \textit{splats} $\Phi$
respectivo ao conjunto de pontos originais, uma lista adicional de índices
$\{f_i\}$ dos pontos originais é mantida para cada \textit{splat} $\mathbf{s}_i$
onde um índice ${i}$ refere a um ponto $\mathbf{p}_i$. 

\begin{figure}[ht]
\centering
\includegraphics[width=15.0cm]{img/cap04/PosterProgressiveThese} 
\caption{Junção dos \textit{splats} de acordo com a métrica
$\mathbf{\mathit{L}}^2$ (esquerda) e $\mathbf{\mathit{L}}^{2,1}$ (direita) }
\label{fig:merge}
\end{figure}  

Então o erro da junção
de dois \textit{splats} $\mathbf{s}_a$ e $\mathbf{s}_b$ em um novo
\textit{splats} $\mathbf{s}_m$ é definido como:
\begin{equation}
 \mathbf{\varepsilon}_{\Phi}  =  ||e||\cdot\mathbf{\sum}_{f \in \{f_m\}}
 |dist(\mathbf{p}_f,\mathbf{s}_m)|^2 ,  \{f_m\} = {f_a} + {f_b}. 
\label{eq:l2}
\end{equation}
Note que o erro é ponderado pela tamanho da aresta, afim de penalizar a junção
de \textit{splats} muito distantes
 
Dado a métrica de erro~\ref{eq:l2} e dois \textit{splats}
$\mathbf{s}_a$ e $\mathbf{s}_b$ para a junção, $\mathbf{s}_m$ é determinado
aplicando-se \textit{PCA} no conjunto de pontos $\mathbf{\mathcal{P}}_m = \{\mathbf{p}_f\}$ diretamente
em $3D$. Assim , teremos o ponto médio $\bar{\mathbf{p}}$ e três autovalores
$\lambda_1 \geq \lambda_2 \geq \lambda_3$ e seus autovetores correspondentes
$\mathbf{v}_1$, $\mathbf{v}_2$, $\mathbf{v}_3$.

Assim, $\mathbf{s}_m$ será
determinado com centro $\mathbf{c}_m = \bar{\mathbf{p}}$, normal $\mathbf{n}_m
= \mathbf{v}_3$ e os dois eixos $\mathbf{t}_m^1$ e $\mathbf{t}_m^2$ com
direções de terão as $\mathbf{v}_1$ e $\mathbf{v}_2$ com tamanho respectivo há
$\sqrt{\lambda_1/\lambda_2}$. Então os tamanho dos eixos serão ajustados de
forma que a \textit{splat} englobe todo os pontos $\mathbf{\mathcal{P}}_m$
quando projetados em $2D$ no plano definido por  $\mathbf{s}_m$~\ref{fig:merge}.  

\subsubsection{Métrica $\mathbf{\mathit{L}}^{2,1}$}
A métrica de erro $\mathbf{\mathit{L}}^{2,1}$ mede o desvio na direção da
normal e foi entendida da métrica original proposta em~\cite{Cohen2004}. Neste
caso o computo é simples e não há necessidade da lista de índices. Dada a
operação de junção $\Phi$ e dois \textit{splats} $\mathbf{s}_a$ e
$\mathbf{s}_b$, com suas respectivas áreas $|\mathbf{s}_a|$ e
$|\mathbf{s}_b|$, similar a~\ref{eq:l2}, o erro ponderado pelo tamanho da
aresta é definido como:
\begin{equation}
 \mathbf{\varepsilon}_{\Phi}  =  ||e||\cdot|\mathbf{s}_a| +
 |\mathbf{s}_b|\cdot|| \mathbf{n}_a - \mathbf{n}_b ||^2.
\label{eq:l21}
\end{equation}

De acordo com a métrica  $\mathbf{\mathit{L}}^{2,1}$ o centro do novo
\textit{splat} $\mathbf{s}_m$ é definido como:
\begin{equation}
 \mathbf{c}_{m}  =  \frac{|\mathbf{s}_a|\cdot\mathbf{c}_{a}  +
 |\mathbf{s}_b|\cdot\mathbf{c}_{b}}{|\mathbf{s}_a|+|\mathbf{s}_b|} 
\label{eq:l21c}
\end{equation}
e normal
\begin{equation}
 \mathbf{n}_{m}  =  \frac{|\mathbf{s}_a|\cdot\mathbf{n}_{a} +  
 |\mathbf{s}_b|\cdot\mathbf{n}_{b}}{|\mathbf{s}_a|+|\mathbf{s}_b|} 
\label{eq:l21n}
\end{equation}

Os vetores $\mathbf{t}_m^1$ e $\mathbf{t}_m^2$ são calculados da mesma forma
que $\mathbf{\mathit{L}}^{2}$, só que ao invés de projetos o conjunto de pontos
$\mathbf{\mathcal{P}}_m$(que não é mais mantido), $n$ pontos ($8$ é o
suficiente) são calculados na borda dos \textit{splats} $|\mathbf{s}_a|$ e $|\mathbf{s}_b|$ e projetados no
plano definido por $|\mathbf{s}_m|$. Só então aplica-se \textit{PCA} e
computa-se os tamanhos de $\mathbf{t}_m^1$ e $\mathbf{t}_m^2$.

Com as duas métricas de erro a sequência de operações de junção de
\textit{splats} ${\Phi_i}$ e sua operação inversa, operação de divisão do
\textit{splat}, podem ser criadas durante o processo de simplificação. Esse
procedimento é similar ao proposto para malhas poligonais~\cite{Hoppe1996} com a
grande diferença de não manter informação de topologia.

\section{Discussão}