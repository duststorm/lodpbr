\chapter{Simplificação de Superfícies}
\label{cap04}
Como mencionado no capítulo~\ref{cap:02}, modelos baseados em pontos complexos
podem chegar a milhões ou mesmo bilhões de amostras, como por exemplo no projeto do David Digital\cite{Levoy2000}. 
Para que seja possível processar ou visualizar estes modelos em tempo real, muitas vezes é necessário reduzir a complexidade realizando operações de simplificação.
Estas técnicas geram aproximações adequadas do modelo reduzindo o número de amostras e procurando manter da melhor forma possível as características do modelo original.
Formalmente, a definição de uma simplificação de superfícies baseadas em pontos
pode ser formulada da seguinte forma: 

Seja uma superfície $\mathbf{\mathcal{S}}$
definida por uma nuvem de pontos $\mathbf{\mathcal{P}}$. Dado um número
amostras $n < |\mathbf{\mathcal{P}}|$, compute uma nuvem de pontos
$\mathbf{\mathcal{P}}^\prime$, onde $|\mathbf{\mathcal{P}^}^\prime| = n$, tal que
a distância $\mathbf{\varepsilon} =
\mathbf{\mathit{d}}(\mathbf{\mathcal{S}},\mathbf{\mathcal{S}}^\prime)$ da
superfície simplificada $\mathbf{\mathcal{S}}^\prime$ da superfície original 
$\mathbf{\mathcal{S}}$ seja mínima.

%Na prática ,  achar um ótimo global do problema descrito acima é $np - hard$
Diversos algoritmos de simplificação utilizam
diferentes heurísticas baseadas em erros locais para encontrar a melhor aproximação.
Neste capítulo serão apresentados alguns métodos de simplificação divididos em duas categorias:
superfícies baseadas em pontos \textit{Puros} e baseadas em \textit{Splats}.

%{New point based simplification based on MLS and splat for Point Modesl}

\section{Simplificação Baseada em Pontos \textit{Puros}}
\label{sec:puro}
Uma abordagem simples e muito usada na simplificação de nuvens de pontos é
avaliar a importância de cada ponto. 
Esta importância é quantificada em um valor que indica a quantidade de informação que o ponto possui para formar a superfície, ou a sua redundância. 
Linsen \cite{Linsen2001} defini a informação contida em um ponto como uma soma ponderada de fatores estimados pelos seus vizinhos, como distância e curvatura por exemplo.
Em Alexa et al.\cite{Alexa2001} os pontos são removidos de acordo com a importância de sua contribuição
na superfície \texit{MLS} ( \texit{Moving Least Squares}\abbrev{MLS}{\texit{Moving Least Squares}}).
Kalaiah e Varshney \cite{Kalaiah2003} medem a importância de cada ponto baseado nas propriedades 
diferenciais fornecidas pela sua vizinhança. 
Após a determinação da importância de cada ponto, a simplificação é então realizada removendo os pontos de baixa contribuição ou alta reduncância. 

\begin{figure}[ht]
\centering
\begin{tabular}{cc}
\includegraphics[width=7.0cm]{img/cap04/teaPotSong} &
\includegraphics[width=6.0cm]{img/cap04/pecaSong} 
\end{tabular}
\caption{Método de simplificação proposto por~\cite{Song2009}. Para os dois
modelos exemplos são mostrados: o modelo original (\textbf{topo-esquerda}), os pontos classificados como de bordas destacados (\textbf{topo-direita}), e o modelo simplificado (\textbf{baixo}).}
\label{fig:song2009}
\end{figure}

Os métodos de simplificação citados acima são baseados na suposição de que a
superfície determinada pela nuvem de pontos seja suave. Contudo, modelos de peças mecânicas por exemplo, contêm descontinuidades que são geralmente bordas afiadas separando a superfície em duas
partes distintas. Song e Feng \cite{Song2009} abordaram este problema criando um método de
simplificação baseado na mesma ideia do métodos
citados acima, porém com duas diferenças principais.
Primeiro, ao invés de operar sobre toda a nuvem de pontos,
o algoritmo considera apenas uma região suave distinta por vez.
Segundo, o grau de importância dos pontos é medido de forma diferente: enquanto Alexa et al.\cite{Alexa2001}
focam em uma simplificação
contínua, e Kalaiah e Varshney \cite{Kalaiah2003} na renderização e em
uma distribuição homogênea, Song e Feng \cite{Song2009} buscam uma simplificação
que preserve as partes característica do modelo, como ilustrado na Figura~\ref{fig:song2009}.

%\begin{figure}[ht]
%\centering
%\includegraphics[width=15.0cm]{img/cap04/MaxPlanckPauly} 
%\caption{\textbf{Topo-Esquerda:} \ldots \textbf{Baixo-Esquerda:} \ldots
%\textbf{Topo-Direita:} \ldots \textbf{Baixo-Direta:}}
%\label{fig:maxplanck}
%\end{figure} 



\begin{figure}[ht]
\centering
\begin{tabular}{cc}

	\begin{minipage}[b]{0.45\linewidth}
    	\centering
 	
\includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinCluster}\\[0cm](a)
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.45\linewidth}
    	\centering
 	
\includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinBSP}\\[0cm](b)
   \end{minipage}
	\\
	
  \begin{minipage}[b]{0.45\linewidth}
  		\centering
       
\includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinParticulas}\\[0cm]
(c)
  \end{minipage}  
  \hfill
  \centeringmaxPlanckWinCluster
  \begin{minipage}[b]{0.45\linewidth}
    	\centering
       
\includegraphics[width=1.00\linewidth]{img/cap04/maxPlanckWinParticulasQEM}\\[
0cm](d)
  \end{minipage}
	
\end{tabular}
\caption{Métodos de simplificação propostos por~\cite{Pauly2002}. \textbf{(a)}
Clusterização Incremental. \textbf{(b)} Clusterização Hierárquica. \textbf{(c)} 
Simulação de Partículas. \textbf{(d)} Simplificação Interativas.}
\label{fig:pauly}
\end{figure}
 

%Moenning e Dodgson \cite{Moenning03} aplicou um método que usa uma abordagem
%progressiva que começando com um ponto escolhido randomicamente, acrescentamos
%a cada passo no conjunto de representantes já selecionados a partir de amostras
%que mais distancie deles. Esse ponto é sabidamente um vértices do driagrama de
%Voronoi (geodésico) do conjunto de pontos já selecionados. Como a determinação
%desse diagrama é custosa ele os autores propoe substitui-lo por uma
%aproximação usando \textit{Fast-Marching Farthest Point} que é descrito a
%seguir:
%\begin{enumerate}
%\item Em primeiro lugar se imerge a nuvem de pontos em um grade regular.
%\item A distância dessa grade ao conjunto de pontos é calculada de forma
%iterativa a partir da distância dos vértices vizinhos que estão mais próximos
%\item da superfície $\mathbf{\mathcal S}$ que ele.
%\item Células contendo vértices cujos pontos mais próximos em $\mathbf{\mathcal
%S}$ são diferentes, contém faces do diagrama de Voronoi. Dentro dessas células
%essas faces são aproximados por um polígono planar.
%\end{enumerate} 

%Na verdade só precisamos tratar as células adjacentes ao vértice mais distante
%de $\mathbf{\mathcal S}$.

%aplicou o princípio de \textit{Fast-Marching Farthest
%Point}, introduzida por Eldar et al.\cite{Eldar1997} no contexto de amostragem
%em
%imagens. O método ultiliza um abordagem progressiva que começa com 
 
 
 %que não requer nenhuma reconstrução de superfícies a priore. O
 %Algoritmo pode gerar uma representação em multiresolução e
 %progressiva de forma eficiente.

Os algoritmos citados acima criam uma nuvem de pontos simplificada realizando uma amostragem do modelo, ou seja, definindo um subconjunto da nuvem original ao remover iterativamente pontos de acordo com uma métrica de erro.
Enquanto estes métodos são simples e eficientes, suas estratégias de
simplificação não garantem uma distribuição global uniforme. Além disso, os
erros propostos por estes métodos por vezes não produzem um boa estimativa para todos casos, não sendo trivial formular métricas genéricas mais precisas.

Uma maneira sistemática de simplificação de pontos foi
proposta por Pauly et al.\cite{Pauly2002}, onde vários métodos de simplificação
de polígonos foram adaptados para pontos. São eles:
\begin{itemize}
  \item{\textit{Métodos de Clusterização:} Onde a nuvem de pontos
  $\mathbf{\mathcal P}$ é dividida em subconjuntos. Os pontos deste subconjunto são então substituídos por um ponto representativo (geralmente seu centróide). Existem dois tipos de clusterização: incremental, onde clusters
 são criados sistematicamente ao crescer regiões através da agregação de pontos
vizinhos; e o hierárquico, onde clusters são criados tomando a nuvem de pontos original e dividindo-a em conjuntos menores.}
  \item{\textit{Simplificação Iterativa:} Onde repetidamente seleciona-se pares
  de pontos juntando-os em um só. Note que para cada junção, há um acréscimo no
  erro total, no entanto, cria-se uma lista ordenada com todos pares possíveis, colocando no topo a junção que provoca o menor erro. Pares de pontos são então aglomerados de forma a minizar o erro. Para avaliar o
  erro de junção dos pares de pontos é usada uma adaptação do \texit{Quadric
  Error Metric}\abbrev{QEM}{\texit{Quadric Error Metric}} apresentado para
  malhas poligonais em~\cite{Garland1997}.}
  \item{\textit{Simulação de Partículas:} Onde há uma distribuição aleatória
  de um número $\mathbf{n}$ de pontos na superfície, tal que
  $|\mathbf{\mathcal{P}^}^\prime| = n$. Cada ponto é considerado como uma
partícula que aplica forças de repulsão às vizinhas, causando um espalhamento até que a
  distribuição desejada seja alcançada.}
\end{itemize}
  
Como discutido em \cite{Pauly2002}, o método de clusterização incremental é o
que possui o maior erro médio, seguido pelo método hierárquico, simulação de
partículas e do método iterativo, que possui menor erro. A distribuição dos
pontos produzida pelos métodos de clusterização se aproxima da distribuição
original, que é desejável em alguns caso. Entretanto, simulação de
partículas é a alternativa mais viável quando se deseja uma densidade uniforme ou
um controle da densidade local da distribuição. Métodos de clusterização são os mais
eficientes devido a sua simplicidade. Clusterização incremental, simplificação iterativa e simulação de partículas possuem custo de armazenamento linear em relação ao número total de amostras, requerindo muitas vezes uma capacidade grande de memória, enquanto a clusterização hierárquica depende apenas do tamanho do modelo simplificado. O autor discute que as três técnicas operam diretamente na nuvem de pontos ao invés de reconstruir a superfície a priori para aplicar o processo de simplificação. A Figura~\ref{fig:pauly} ilustra estes métodos.

\section{Simplificação Baseada em \textit{Splats}}
\label{cap04:sec:splats}
Como discutido no capítulo \ref{cap:02}, para preencher buracos entre os pontos
de forma mais eficiente, representações baseadas em pontos são frequentemente
estendidas para representações baseadas em \textit{Splats}~\cite{Zwicker2001},
onde a superfície é aproximada por discos ou elipses. Esta representação é de
especial interesse no contexto de renderização, onde os \textit{splats}
facilitam a concepção de algoritmos eficientes~\cite{Ren2002,BotschW2002,Botsch2003,marroquim07} que gerem imagens de alta qualidade.

Embora a representação baseada em \textit{splats} seja bastante utilizada na
prática, muitos esquemas de simplificação tomam apenas o centro do \textit{splat} em consideração (como discutido na seção anterior).
A ideia é primeiro gerar o conjunto de pontos na qual a densidade se adapta a
curvatura da superfície, e em um processo posterior converter os pontos
em \textit{splats} estimando sua normal e tamanho pela análise local de sua vizinhança. 
Em contraste, técnicas de simplificação
baseadas em \textit{splats}, exploram toda a geometria do disco ou elipse para
obter o subconjunto que recobre a superfície. No entanto, estas
técnicas são computacionalmente mais intensas.

Wu e Kobbelt~\cite{WuK04} desenvolveram uma técnica de simplificação
otimizada baseada em \textit{splats} que explora a flexibilidade de modelos de
pontos, ou seja, sem informação de topologia. Utilizando um esquema de otimização
global é computada uma
aproximação de toda a superfície com um conjunto mínimo de \textit{splats} que
satisfaz um erro global pré-determinado. 


\begin{figure}[ht]
\centering
\includegraphics[width=15.0cm]{img/cap04/wu04Iteractive} 
\caption{ \textbf{Esquerda:} Modelo de um torso feminino $171.000$ pontos.
\textbf{Meio:} Modelo aproximado por $422$ \textit{splats} após o segundo passo
(seleção gulosa). \textbf{Direita:} $333$ \textit{splats} após
finalizado o terceiro passo (otimização global usando sistemas de partículas)
A segunda e quarta imagem mostram os \textit{splats} renderizados usando filtro EWA. 
Note a melhor distribuição dos
\textit{splats} após passo de otimização global}
\label{fig:wu04}
\end{figure} 

O objetivo da simplificação otimizada é computar um conjunto mínimo de
\textit{splats} $\mathbf{\mathcal{T}=\{\mathbf{s}_j}\}$ que aproxime o conjunto
original $\mathbf{\mathcal{P}}$ com um erro abaixo de uma tolerância
$\mathbf{\varepsilon}$. Em uma fase de pré-processamento propriedades locais da superfície, como vetor normal $\mathbf{n}_i$ e 
densidade $\mathbf{\omega}_i$, são calculadas para cada ponto ${\mathbf{p}_i}$ baseadas em uma vizinhaça local. Dependendo da
aplicação, pode-se decidir entre \textit{splats} circulares ou elípticos. Um
\textit{splat} circular é definido por um centro ${\mathbf{c}_i}$, normal
${\mathbf{n}_i}$ e um raio ${\mathbf{r}_i}$; no caso do elíptico o raio
${\mathbf{r}_i}$ é substituído por dois vetores ${\mathbf{t}_i^1}$ e
${\mathbf{t}_i^2}$ que definem os seus eixos principais.
Para assegurar o erro $\mathbf{\varepsilon}$ máximo, uma nova métrica de distância
\textit{splat}-ponto é introduzida: para um ponto
${\mathbf{p}_i}$ a distância ao \textit{splat} ${\mathbf{\mathcal T}}$ é
definida como sendo a projeção ortogonal no plano do \textit{splat} ${\mathbf{s}_j}$:
\begin{equation}
 dist(\mathbf{p}_i,\mathbf{\mathcal T}) = dist(\mathbf{p}_i,\mathbf{s}_j) =
 |\mathbf{n}_i^{\mathit T}(\mathbf{p}_i - \mathbf{c}_i)|
\label{eq1}
\end{equation}
\noindent se
\begin{equation}
 || (\mathbf{p}_i - \mathbf{c}_i) - \mathbf{n}_j^{\mathit T}(\mathbf{p}_i -
 \mathbf{c}_i)\mathbf{n}_j ||^2 \leq \mathbf r_j ^2
\label{eq:circular}
\end{equation}
no caso de um \textit{splat} circular ou
\begin{equation}
 (  \mathbf{t}_j^{1^{\mathit T}}(\mathbf{p}_i - \mathbf{c}_i) )^2  + 
 (\mathbf{t}_j^{2^{\mathit T}}(\mathbf{p}_i - \mathbf{c}_i))^2 \leq 1
\label{eq:eliptico}
\end{equation}
se for um \textit{splat} elíptico. Se $\mathbf{p}_i$ projetar no interior de vários
\textit{splats} a distância mínima é escolhida. Se nenhum $\mathbf{s}_j$ for encontrado pelas equações \ref{eq:circular} ou \ref{eq:eliptico}, sua distância é definida como $dist(\mathbf{p}_i,\mathbf{\mathcal T}) = \infty$.
 
A simplificação otimizada é realizada em três passos. No primeiro passo é computado o
\textit{splat} máximo para cada ponto ${\mathbf{p}_i}$ cujo erro é limitado por
um $\mathbf{\varepsilon}$. Começando por uma semente $\mathbf{p}_i$, 
o \textit{splat} $\mathbf{s}_i$ cresce ao encorporar os vizinhos
de acordo com sua distâncias. Esta distância é calculada usando as equações \ref{eq:circular} e
\ref{eq:eliptico}, sendo que os \textit{splats} elípticos possuem a vantagem de 
adaptarem-se melhor à curvatura da superfície. Em um segundo passo, para o conjunto
inicial de \textit{splats}, um subconjunto que cobre a superfície sem que haja buracos entre os vizinhos é selecionado
usando um procedimento guloso.
 
Por último, o procedimento guloso anterior é otimizado usando um procedimento
global. A ideia é substituir iterativamente subconjuntos de \textit{splats}
por um novo subconjunto que possua os mesmo elementos ou que tenha uma
distribuição melhor. Para melhorar o procedimento anterior, baseado em decisões locais e
geralmente redundante ou possuindo uma distribuição não uniforme, é utilizado um
sistemas de partículas como em~\cite{Pauly2002,Tuk} porém diferenciado na forma
como as partículas interagem. A figura~\ref{fig:wu04} mostra o
efeito desta simplificação otimizada após a finalização do último passo.

Wu el al.~\cite{ProgressiveSplat2005} desenvolveram um algoritmo iterativo com
um abordagem gulosa para criar uma representação progressiva de \textit{splats}.
O seu funcionamento é semelhante ao \textit{Progressive Meshes} de
Hoppe~\cite{Hoppe1996}, no entanto, são criados
\textit{splats} para cada conjunto de pontos. Em seguida, todas as possíveis operações de
\textit{junção de} \texti{splats} são organizada em uma lista de prioridades de
acordo com uma \textit{métrica de
erro}, que avalia o erro causado pela respectiva operação de junção.

\begin{figure}[ht]
\centering
\includegraphics[width=15.0cm]{img/cap04/progressiveSplat} 
\caption{ Representação progressiva do modelo de Carlos Magno ($600.000$
 de pontos). Da esquerda para direita, modelos com: $2.000$,
$10.000$, $70.000$ e $600.000$ \textit{splats}}
\label{fig:progressiveSplat}
\end{figure} 

A entrada do algoritmo é um conjunto de pontos $\mathbf{\mathcal P}
= \{\mathbf{1}\ldots\mathbf{p}_i\}$ que são transformados em um conjunto de
\textit{splats} $\mathbf{\mathcal T  = \{\mathbf{1} \ldots \mathbf{s}_i}\}$.
Cada \textit{splat} representa uma elipse em $3D$ com centro $\mathbf{c}_i$,
normal
$\mathbf{n}_i$ e dois vetores ortogonais $\mathbf{t}_i^1$ e $\mathbf{t}_i^2$.

Similar a~ \cite{Pauly2002} e \cite{WuK04} os $k$-vizinhos de $\mathbf{p}_i$, 
$\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$,   
são computados previamente para analisar localmente a superfície 
(computar normal $\mathbf{n}_i$) e gerar o splat inicial $\mathbf{s}_i$.
Para tanto, um plano $\mathbf{\mathcal{H}}$ é formado para $\mathbf{p}_i$ e
$\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$ definindo $\mathbf{s}_i$ com normal 
$\mathbf{n}_i$ e centro $\mathbf{c}_i = \mathbf{p}_i$. Como os
\textit{splats} iniciais são círculos, $\mathbf{t}_i^1$ e $\mathbf{t}_i^2$
serão dois vetores ortogonais  paralelos a $\mathbf{\mathcal{H}}$ com mesmo
tamanho $\mathbf{r}_i$:

\begin{equation}
 \mathbf{r}_i  = \max_j || (\mathbf{p}_j - \mathbf{c}_i) - \mathbf{n}_i^{\mathit
 T}(\mathbf{p}_j - \mathbf{c}_i)\mathbf{n}_i || ,
\label{eq:planocircular}
\end{equation}

\noindent para todo  $\mathbf{p}_j$ $\in$
$\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$. Assim como feito por
\cite{Pauly2002}, $\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$ será usado para criar
uma topologia dinâmica para auxiliar o processo de junção de \textit{splats}.
Sendo assim, é criado um grafo
$\mathbf{\mathcal{G}}(\mathbf{\mathcal{P}},\mathit{E})$ onde uma aresta $(i,j)$ 
pertence a $\mathit{E}$ se $\mathbf{p}_j \in
\mathbf{\mathcal{N}}_k(\mathbf{p}_i)$.
Então uma operação de junção $\Phi$ juntará dois
\textit{splats} $\mathbf{s}_a$ e $\mathbf{s}_b$ associados a dois pontos
$\mathbf{p}_a$ e $\mathbf{p}_b$ de um aresta $\mathbf{e} \in \mathit{E}$
em um \textit{splat} $\mathbf{s}_m$. A lista de prioridades conterá
todas as
possíveis operações de junção de todas as arestas pertencentes a $\mathit{E}$.  
Para avaliar a operação de junção de dois \texit{splats}, dois erros foram
entendidos. Um para medir o erro relativo à distância ($\mathbf{\mathit{L}}^2$) e
outro ao desvio da normal ($\mathbf{\mathit{L}}^{2,1}$).

\subsubsection{Métrica $\mathbf{\mathit{L}}^2$}
\label{cap04:sec:splats:l2}
A métrica $\mathbf{\mathit{L}}^2$ é baseada na distância Euclidiana. Para
computar o erro causado pela operação $\Phi$
em relação ao conjunto de pontos originais, uma lista adicional de índices
$\{f_i\}$ dos pontos originais é mantida para cada \textit{splat} $\mathbf{s}_i$,
onde o índice ${i}$ refere-se ao ponto $\mathbf{p}_i$. 

\begin{figure}[ht]
\centering
\includegraphics[width=15.0cm]{img/cap04/PosterProgressiveThese} 
\caption{Junção dos \textit{splats} de acordo com a métrica
$\mathbf{\mathit{L}}^2$ (esquerda) e $\mathbf{\mathit{L}}^{2,1}$ (direita) }
\label{fig:merge}
\end{figure}  

O erro da junção
de dois \textit{splats} $\mathbf{s}_a$ e $\mathbf{s}_b$ em um novo
\textit{splats} $\mathbf{s}_m$ é definido como:
\begin{equation}
 \mathbf{\varepsilon}_{\Phi}  =  ||e||\cdot\mathbf{\sum}_{f \in \{f_m\}}
 |dist(\mathbf{p}_f,\mathbf{s}_m)|^2 ,  \{f_m\} = {f_a} + {f_b}. 
\label{eq:l2}
\end{equation}
Note que o erro é ponderado pelo tamanho da aresta, a fim de penalizar a junção
de \textit{splats} muito distantes.
 
Dado a métrica de erro~\ref{eq:l2} e dois \textit{splats}
$\mathbf{s}_a$ e $\mathbf{s}_b$ para a junção, $\mathbf{s}_m$ é determinado
pela método de análise das componentes principais (\textit{PCA}) do conjunto de pontos $\mathbf{\mathcal{P}}_m =
\{\mathbf{p}_f\}$ diretamente
em $3D$. Assim, teremos o ponto médio $\bar{\mathbf{p}}$ e três autovalores
$\lambda_1 \geq \lambda_2 \geq \lambda_3$ e seus autovetores correspondentes
$\mathbf{v}_1$, $\mathbf{v}_2$, $\mathbf{v}_3$.

Desta forma, $\mathbf{s}_m$ será
determinado com centro $\mathbf{c}_m = \bar{\mathbf{p}}$, normal $\mathbf{n}_m
= \mathbf{v}_3$ e os dois eixos $\mathbf{t}_m^1$ e $\mathbf{t}_m^2$ definidos
pelas direções de $\mathbf{v}_1$ e $\mathbf{v}_2$ com tamanho respectivo a
$\sqrt{\lambda_1/\lambda_2}$. Os tamanho dos eixos são ajustados de
forma que a \textit{splat} englobe todo os pontos $\mathbf{\mathcal{P}}_m$
quando projetados no plano definido por $\mathbf{s}_m$ (Figura~\ref{fig:merge}).
 

\subsubsection{Métrica $\mathbf{\mathit{L}}^{2,1}$}
\label{cap04:sec:splats:l21}
A métrica de erro $\mathbf{\mathit{L}}^{2,1}$ mede o desvio na direção da
normal, sendo uma extensão da métrica original proposta em~\cite{Cohen2004}. Neste
caso o computo é simples e não há necessidade da lista de índices. Dada a
operação de junção $\Phi$ e dois \textit{splats} $\mathbf{s}_a$ e
$\mathbf{s}_b$, com suas respectivas áreas $|\mathbf{s}_a|$ e
$|\mathbf{s}_b|$, similar a~\ref{eq:l2}, o erro ponderado pelo tamanho da
aresta é definido como:
\begin{equation}
 \mathbf{\varepsilon}_{\Phi}  =  ||e||\cdot|\mathbf{s}_a| +
 |\mathbf{s}_b|\cdot|| \mathbf{n}_a - \mathbf{n}_b ||^2.
\label{eq:l21}
\end{equation}

De acordo com a métrica  $\mathbf{\mathit{L}}^{2,1}$ o centro do novo
\textit{splat} $\mathbf{s}_m$ é definido como:
\begin{equation}
 \mathbf{c}_{m}  =  \frac{|\mathbf{s}_a|\cdot\mathbf{c}_{a}  +
 |\mathbf{s}_b|\cdot\mathbf{c}_{b}}{|\mathbf{s}_a|+|\mathbf{s}_b|} 
\label{eq:l21c}
\end{equation}
e normal
\begin{equation}
 \mathbf{n}_{m}  =  \frac{|\mathbf{s}_a|\cdot\mathbf{n}_{a} +  
 |\mathbf{s}_b|\cdot\mathbf{n}_{b}}{|\mathbf{s}_a|+|\mathbf{s}_b|}.
\label{eq:l21n}
\end{equation}

Os vetores $\mathbf{t}_m^1$ e $\mathbf{t}_m^2$ são calculados da mesma forma
que para métrica $\mathbf{\mathit{L}}^{2}$, porém, como o conjunto de pontos
$\mathbf{\mathcal{P}}_m$ não é mais mantido para ser projetado, $n$ pontos (geralmente $8$ é o
suficiente) são calculados na borda dos \textit{splats} $|\mathbf{s}_a|$ e
$|\mathbf{s}_b|$ e projetados no
plano definido por $|\mathbf{s}_m|$. A partir destes aplica-se \textit{PCA} e
computa-se os tamanhos de $\mathbf{t}_m^1$ e $\mathbf{t}_m^2$.

Com as duas métricas de erro a sequência de operações de junção de
\textit{splats} ${\Phi_i}$ e sua operação inversa, operação de divisão do
\textit{splat}, podem ser criadas durante o processo de simplificação. Esse
procedimento é similar ao proposto para malhas poligonais~\cite{Hoppe1996}, 
porém sem manter informação de topologia.

\section{Discussão}